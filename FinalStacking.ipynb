{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.stats import iqr\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final part of the capstone project where we build a simple stacker that combines the best XGBoost and MLP models.\n",
    "\n",
    "The main idea for stacking was derived from here: https://www.kaggle.com/general/18793\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "The methodlogy for stacking is as follows: \n",
    "\n",
    "- **Splitting the dataset**: Split the train dataset into k folds\n",
    "- **out-of-fold predictions**: Fit first stage models(Level 0 or L0) on k-1 folds and predict the kth fold\n",
    "- ** full set prediction**: Fit first stage models on the whole train set and get the predictions for the test dataset. The predictions from each model  are combined into a test set where each feature is respective L0 model predicitons \n",
    "- ** stacker model**: Using the predictions from the out-of-fold set and labels of the training set , fit a stacker model (L1 model). Finally, the stacker model is used to predict test set labels based on the on the combined testset of L0 model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Splitting of dataset\n",
    "\n",
    "The dataset for XGBoost and MLP model is prepared by following same steps follwed in the respective model implementation stage.\n",
    "\n",
    "- XGBoost Model: Label encoding the categorical features\n",
    "- MLP Model: One-hot encoding for the binary features and label encoding for other cat variables\n",
    "\n",
    "Also, the test set for L1 and L0 models is prepared by splitting the training set into train and test sets as we don't want to touch the actual Kaggle test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data set has (7662, 208) samples\n",
      "Train data set has (21083, 208) features & (21083,) labels\n",
      "Test data set has (9036, 208) features & (9036,) labels\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST model\n",
    "\n",
    "train_xgb = pd.read_csv(\"input/train_fe.csv\")\n",
    "test_xgb = pd.read_csv('input/test_fe.csv')\n",
    "\n",
    "final_test_xgb_id_test = test_xgb['id']\n",
    "\n",
    "#Preparing test data\n",
    "for col in test_xgb.columns:\n",
    "    if test_xgb[col].dtype == 'object':\n",
    "        \n",
    "        lbl.fit(test_xgb[col].values)\n",
    "        test_xgb[col] = lbl.transform(test_xgb[col].values)\n",
    "   # else:\n",
    "        #test_xgb[col] = test_xgb[col]\n",
    "            \n",
    "        \n",
    "final_test_xgb_X = test_xgb.drop(['id', 'timestamp'], axis=1)\n",
    "\n",
    "#Show the results \n",
    "print(\"Test data set has {} samples\".format(final_test_xgb_X.shape))\n",
    "\n",
    "\n",
    "# Data preprocessing and creating of test-train split of Train DATA\n",
    "for col in train_xgb.columns:\n",
    "    if train_xgb[col].dtype == 'object':\n",
    "        lbl.fit(train_xgb[col].values)\n",
    "        train_xgb[col] = lbl.transform(train_xgb[col].values)\n",
    "        \n",
    "# Training data test-train set preparation \n",
    "train_xgb_y = train_xgb['price_doc']\n",
    "train_xgb_X = train_xgb.drop(['id', 'timestamp','price_doc'],axis=1)\n",
    "\n",
    "train_xgb_y = np.array(train_xgb_y)\n",
    "train_xgb_X = np.array(train_xgb_X)\n",
    "# test-train splitting\n",
    "xgb_train_X, xgb_test_X, xgb_train_y, xgb_test_y = train_test_split(train_xgb_X, train_xgb_y, test_size=0.3, random_state=0)\n",
    "#Show the results\n",
    "print(\"Train data set has {} features & {} labels\".format(xgb_train_X.shape,xgb_train_y.shape ))   \n",
    "print(\"Test data set has {} features & {} labels\".format(xgb_test_X.shape,xgb_test_y.shape ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN model\n",
    "\n",
    "# importing train_mlp and test_mlp datasets\n",
    "train_mlp=pd.read_csv(\"input/train_fe.csv\")\n",
    "test_mlp=pd.read_csv(\"input/test_fe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data set has (7662, 353) samples\n",
      "Train data set has (21083, 353) features & (21083,) labels\n",
      "Test data set has (9036, 353) features & (9036,) labels\n"
     ]
    }
   ],
   "source": [
    "cols = train_mlp.columns\n",
    "train_mlp.shape, test_mlp.shape\n",
    "\n",
    "# marking train_mlp and test_mlp sets\n",
    "n=train_mlp.shape[0]\n",
    "train_mlp['data_set']=1\n",
    "test_mlp['data_set']=0\n",
    "test_mlp.price_doc=np.nan\n",
    "\n",
    "final_test_mlp_id_test=test_mlp['id']\n",
    "\n",
    "#logarithmic transformation of target did not better results\n",
    "train_mlp.price_doc=(train_mlp.price_doc)\n",
    "target_mlp=train_mlp.price_doc\n",
    "train_mlp=train_mlp.append(test_mlp)\n",
    "train_mlp.drop(['id'],axis=1,inplace=True)\n",
    "\n",
    "# listing down binary features\n",
    "binary=[]\n",
    "for i in train_mlp:\n",
    "    if train_mlp[i].dtypes=='object':\n",
    "        #print(train_mlp[i].value_counts())\n",
    "        if train_mlp[i].value_counts().shape[0]==2:\n",
    "            binary.append(i)\n",
    "\n",
    "# converting categorical varibales and handling missing data \n",
    "for i in binary:\n",
    "    train_mlp[i]=pd.factorize(train_mlp[i])[0]\n",
    "'''train_mlp.loc[train_mlp['ecology']=='no data','ecology_dat']=0\n",
    "train_mlp.loc[train_mlp['ecology']!='no data','ecology_dat']=1\n",
    "train_mlp.loc[train_mlp['ecology']=='no data','ecology']=np.nan\n",
    "train_mlp.loc[train_mlp['ecology']=='poor','ecology']=1\n",
    "train_mlp.loc[train_mlp['ecology']=='satisfactory','ecology']=2\n",
    "train_mlp.loc[train_mlp['ecology']=='good','ecology']=3\n",
    "train_mlp.loc[train_mlp['ecology']=='excellent','ecology']=4\n",
    "train_mlp.ecology=pd.to_numeric(train_mlp.ecology)'''\n",
    "train_mlp=pd.concat([train_mlp,pd.get_dummies(train_mlp.sub_area)],axis=1)\n",
    "\n",
    "a=train_mlp.describe()\n",
    "for i in a:\n",
    "    train_mlp[i]=train_mlp[i].fillna((a.loc['min',i]-a.loc['max',i]*2))\n",
    "    \n",
    "train_mlp.drop(['timestamp','sub_area'],inplace=True,axis=1)\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cols=train_mlp.columns.tolist()\n",
    "\n",
    "train_mlp = pd.DataFrame(scaler.fit_transform(train_mlp), columns=cols)\n",
    "\n",
    "# separating train_mlp and test_mlp after preprocessing\n",
    "\n",
    "test_mlp=train_mlp[train_mlp['data_set']==0]\n",
    "train_mlp=train_mlp[train_mlp['data_set']==1]\n",
    "\n",
    "test_mlp.drop(['data_set','price_doc',],inplace=True,axis=1)\n",
    "train_mlp.drop(['data_set','price_doc'],inplace=True,axis=1)\n",
    "\n",
    "final_test_mlp_X = test_mlp\n",
    "print(\"Test data set has {} samples\".format(final_test_mlp_X.shape))\n",
    "\n",
    "train_mlp = np.array(train_mlp)\n",
    "target_mlp = np.array(target_mlp)\n",
    "\n",
    "# test-train splitting\n",
    "mlp_train_X, mlp_test_X, mlp_train_y, mlp_test_y = train_test_split(train_mlp, target_mlp, test_size=0.3, random_state=0)\n",
    "#Show the results\n",
    "print(\"Train data set has {} features & {} labels\".format(mlp_train_X.shape,mlp_train_y.shape ))   \n",
    "print(\"Test data set has {} features & {} labels\".format(mlp_test_X.shape,mlp_test_y.shape )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the L0-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_xgb = xgb.XGBRegressor(num_boost_round=100,\n",
    "                           eta=0.1,\n",
    "                           eval_metric='rmse',\n",
    "                           objective='reg:linear',\n",
    "                           seed=0,\n",
    "                           gamma=0,\n",
    "                           max_depth=8,\n",
    "                           min_child_weight=6,\n",
    "                           colsample_bytree=0.6,\n",
    "                           subsample=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost\n",
    "\n",
    "folds = KFold(len(xgb_train_y), shuffle=False, n_folds=3)\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(folds):\n",
    "    xtr = xgb_train_X[train_index]\n",
    "    ytr = xgb_train_y[train_index]\n",
    "    xte, yte = xgb_train_X[test_index], xgb_train_y[test_index]\n",
    "\n",
    "    reg_xgb.fit(xtr, ytr)\n",
    "    np.savetxt('ensemble/xgb_pred_fold_{}.txt'.format(k), (reg_xgb.predict(xte)))\n",
    "    np.savetxt('ensemble/xgb_test_fold_{}.txt'.format(k), yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim = mlp_train_X.shape[1],init='normal' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    # Compile Model\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MLP\n",
    "\n",
    "folds = KFold(len(mlp_train_y), shuffle=False, n_folds=3)\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(folds):\n",
    "    xtr = mlp_train_X [train_index]\n",
    "    ytr = mlp_train_y[train_index]\n",
    "    xte, yte = mlp_train_X[test_index], mlp_train_y[test_index]\n",
    "    \n",
    "    reg_mlp = mlp_model()\n",
    "    fit = reg_mlp.fit(xtr, ytr, batch_size=32, epochs=100, verbose=0)\n",
    "    pred = reg_mlp.predict(xte, batch_size=32)\n",
    "    np.savetxt('ensemble/mlp_pred_fold_{}.txt'.format(k), pred)\n",
    "    np.savetxt('ensemble/mlp_test_fold_{}.txt'.format(k), yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the whole dataset\n",
    "\n",
    "<p> We train the same models on the whole training set (xgb_train_X, mlp_train_X and corresponding labels xgb_train_y, mlp_train_y) and generate predictions for the test set (xgb_test_X, mlp_test_X). <br>It is important to note that we do have labels for the test set, but we don't allow our L0-model see them instead it uses from the one of the 3 folds from the training set .\n",
    "<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost on whole set and predicted using test set\n",
    "\n",
    "reg_xgb.fit(xgb_train_X, xgb_train_y)\n",
    "pred_xgb = reg_xgb.predict(xgb_test_X)\n",
    "np.savetxt('ensemble/xgb_pred_test.txt'.format(k), pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#MLP on whole set and predicted using test set\n",
    "\n",
    "reg_mlp = mlp_model()\n",
    "fit = reg_mlp.fit(mlp_train_X, mlp_train_y, batch_size=32, epochs=100, verbose=0)\n",
    "pred_mlp = reg_mlp.predict(mlp_test_X, batch_size=32)\n",
    "np.savetxt('ensemble/mlp_pred_test.txt'.format(k), pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. L1-model training\n",
    "\n",
    "<p>When the previous stage is completed, we have generated out-of-fold and test set predictions, which we can now use to train the stacker.<br/>Firstly, Loading out-of-fold predictions:<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_xgb1 = np.loadtxt('ensemble/xgb_pred_fold_0.txt')\n",
    "train_xgb2 = np.loadtxt('ensemble/xgb_pred_fold_1.txt')\n",
    "train_xgb3 = np.loadtxt('ensemble/xgb_pred_fold_2.txt')\n",
    "\n",
    "train_mlp1 = np.loadtxt('ensemble/mlp_pred_fold_0.txt')\n",
    "train_mlp2 = np.loadtxt('ensemble/mlp_pred_fold_1.txt')\n",
    "train_mlp3 = np.loadtxt('ensemble/mlp_pred_fold_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "\n",
    "<p>Checking if the stacked out-of-fold predictions are within the relavent limits.<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combing the out-of-fold predictions\n",
    "\n",
    "train_xgb_folds = np.hstack((train_xgb1,train_xgb2,train_xgb3))\n",
    "train_mlp_folds = np.hstack((train_mlp1,train_mlp2,train_mlp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2263499.0271548009"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE of XGBoost combined predictions\n",
    "\n",
    "mean_squared_error(xgb_train_y, train_xgb_folds)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2520728.1733623822"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE of MLP combined predictions\n",
    "\n",
    "mean_squared_error(mlp_train_y, train_mlp_folds)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Both the above predictions for the combined out-of-fold predictions have an error within the expected range.  In order to get a baseline score for the stacker, let's calculate predictions with the entire training set.\n",
    "<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_xgb_pred = np.loadtxt('ensemble/xgb_pred_test.txt')\n",
    "test_mlp_pred = np.loadtxt('ensemble/mlp_pred_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE which we need to improve with stacking, XGB: 2238255.3947151075; MLP: 2615654.310512517.\n"
     ]
    }
   ],
   "source": [
    "mse_xgb_test = mean_squared_error(xgb_test_y, test_xgb_pred)**0.5\n",
    "mse_mlp_test = mean_squared_error(mlp_test_y, test_mlp_pred)**0.5\n",
    "print (\"Baseline MSE which we need to improve with stacking, XGB: {}; MLP: {}.\".format(mse_xgb_test, mse_mlp_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The predictions from the whole training data sets a baseline score that the stacker should better\n",
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training L1-model\n",
    "\n",
    "This final step involves combining the L0-models (XGB and MLP). The selection of algorithm is very cruical, a simple regressor is preferable as it reduces the chances of overfitting. Hence, let's select linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_train_x = np.vstack((train_xgb_folds, train_mlp_folds)).T\n",
    "l1_test_x = np.vstack((test_xgb_pred, test_mlp_pred)).T\n",
    "l1_train_y = mlp_train_y \n",
    "l1_test_y = mlp_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (21083, 2)\n",
      "ytrain shape: (21083,)\n",
      "Xtest shape: (9036, 2)\n",
      "ytest shape: (9036,)\n"
     ]
    }
   ],
   "source": [
    "# dimensions \n",
    "print (\"Xtrain shape:\", l1_train_x.shape)\n",
    "print (\"ytrain shape:\", l1_train_y.shape)\n",
    "print (\"Xtest shape:\", l1_test_x.shape)\n",
    "print (\"ytest shape:\", l1_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We now fit a very basic linear regression and get the predictions for the final test set:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2237728.5415377445"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "\n",
    "# Note that normalizing the data in case of linear models is very important\n",
    "reg.fit((l1_train_x), (l1_train_y))\n",
    "reg_pred = (reg.predict((l1_test_x)))\n",
    "\n",
    "mse_stacker = mean_squared_error(l1_test_y, reg_pred)**0.5\n",
    "mse_stacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for XGB: 2238255.39472\n",
      "RMSE for MLP: 2615654.31051\n",
      "RMSE for stacker: 2237728.54154\n"
     ]
    }
   ],
   "source": [
    "print (\"RMSE for XGB:\", mse_xgb_test)\n",
    "print (\"RMSE for MLP:\", mse_mlp_test)\n",
    "print (\"RMSE for stacker:\", mse_stacker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82507246,  0.21255648])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The regression coefficient indicates the final prediction is a linear combination of XGB and MLP models and has following relationship:\n",
    " ### `Final_Prediction = 0.80*XGB_Pred + 0.23*MLP_Pred` \n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final ensemble model has CV error of 2236490.76654, that is best score achived compared to the individual models used in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, xgboost_func, mlp_func, train_path='input/train_fe.csv',test_path= 'input/test_fe.csv',\n",
    "                 seed=0, test_size=0.25, **kwargs):\n",
    "        self.seed = seed\n",
    "        self.test_size = test_size\n",
    "        self.xgboost_func = xgboost_func\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.mlp_func = mlp_func\n",
    "        self.mlp_fit_kwargs = kwargs.get('mlp_fit_kwargs', {'nb_epoch': 30, 'batch_size': 32, 'verbose': 1})\n",
    "        self.mlp_predict_kwargs = kwargs.get('mlp_predict_kwargs', {'batch_size': 32, 'verbose': 1})\n",
    "\n",
    "    def stack_and_compare(self):\n",
    "        xg_xtr, xg_xte, xg_ytr, xg_yte = self.preprocess(encoding='XGB', transform_label=False) #for XGBoost\n",
    "        mlp_xtr, mlp_xte, mlp_ytr, mlp_yte = self.preprocess(encoding='MLP', transform_label=False) # for MLP\n",
    "\n",
    "        xgb_folds = self.predict_folds(self.xgboost_func, xg_xtr, xg_ytr)\n",
    "        mlp_folds = (self.predict_folds(self.mlp_func, mlp_xtr, mlp_ytr,\n",
    "                                              fit_kwargs=self.mlp_fit_kwargs, predict_kwargs=self.mlp_predict_kwargs))\n",
    "\n",
    "        xgb_pred_hold = self.predict_holdout(self.xgboost_func, xg_xtr, xg_ytr, xg_xte)\n",
    "        mlp_pred_hold = (self.predict_holdout(self.mlp_func, mlp_xtr, mlp_ytr, mlp_xte,\n",
    "                                                    fit_kwargs=self.mlp_fit_kwargs,\n",
    "                                                    predict_kwargs=self.mlp_predict_kwargs))\n",
    "\n",
    "        score_xgb, score_mlp = self.evaluate_estimators(xgb_pred_hold, mlp_pred_hold, xg_yte)\n",
    "        print('Single model performance:', 'xgb:', score_xgb, ',', 'mlp:', score_mlp)\n",
    "\n",
    "        stacker = self.stack(xgb_folds, mlp_folds, xg_ytr)\n",
    "        score_stacker = self.evaluate_stacker(stacker, xgb_pred_hold, mlp_pred_hold, xg_yte)\n",
    "\n",
    "        print({'xgb': score_xgb**0.5, 'mlp': score_mlp**0.5, 'stacker': score_stacker**0.5})\n",
    "        return {'xgb': score_xgb**0.5, 'mlp': score_mlp**0.5, 'stacker': score_stacker**0.5}\n",
    "\n",
    "    def preprocess(self, encoding='XGB', transform_label=False):\n",
    "        train = pd.read_csv(self.train_path)\n",
    "        test = pd.read_csv(self.test_path)\n",
    "        if transform_label:\n",
    "            train['price_doc'] = (train['price_doc'])\n",
    "        cat_features = [x for x in train.select_dtypes(include=['object']).columns if x not in ['id', 'price_doc']]\n",
    "        if encoding == 'XGB':\n",
    "            for col in train.columns:\n",
    "                if train[col].dtype == 'object':\n",
    "                    lbl.fit(train[col].values)\n",
    "                    train[col] = lbl.transform(train[col].values)\n",
    "            #print('XGB Label',train['price_doc'].head())\n",
    "            \n",
    "            features = [x for x in train.columns if x not in ['id', 'price_doc']]\n",
    "            train_x = np.array(train[features])\n",
    "            print(train['price_doc'].head(n=2))\n",
    "            train_y = np.array(train['price_doc'])\n",
    "            \n",
    "        elif encoding == 'MLP':\n",
    "            \n",
    "            cols = train.columns\n",
    "            # marking train_mlp and test_mlp sets\n",
    "            n = train.shape[0]\n",
    "            #train['data_set'] = 1\n",
    "            #test['data_set'] = 0\n",
    "            #test.price_doc = np.nan\n",
    "\n",
    "            # logarithmic transformation of target did not better results\n",
    "            #train.price_doc = (train.price_doc)\n",
    "            target_mlp = train.price_doc\n",
    "            #train = train.append(test)\n",
    "            train.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "            # listing down binary features\n",
    "            binary = []\n",
    "            for i in train:\n",
    "                if train[i].dtypes == 'object':\n",
    "                    # print(train_mlp[i].value_counts())\n",
    "                    if train[i].value_counts().shape[0] == 2:\n",
    "                        binary.append(i)\n",
    "\n",
    "            # converting categorical varibales and handling missing data\n",
    "            for i in binary:\n",
    "                train[i] = pd.factorize(train[i])[0]\n",
    "\n",
    "            train = pd.concat([train, pd.get_dummies(train.sub_area)], axis=1)\n",
    "            #NaN treatment\n",
    "            a = train.describe()\n",
    "            for i in a:\n",
    "                train[i] = train[i].fillna((a.loc['min', i] - a.loc['max', i] * 2))\n",
    "\n",
    "            train.drop(['timestamp', 'sub_area'], inplace=True, axis=1)\n",
    "\n",
    "            # from sklearn.preprocessing import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "            cols = train.columns.tolist()\n",
    "\n",
    "            train = pd.DataFrame(scaler.fit_transform(train), columns=cols)\n",
    "\n",
    "            # separating train_mlp and test_mlp after preprocessing\n",
    "\n",
    "            #test = train[train['data_set'] == 0]\n",
    "            #train = train[train['data_set'] == 1]\n",
    "\n",
    "            # test.drop(['data_set' ], inplace=True, axis=1)\n",
    "            #train.drop(['data_set'], inplace=True, axis=1)\n",
    "            \n",
    "            final_test_mlp_X = train\n",
    "            print(\"Train data set has {} samples\".format(final_test_mlp_X.shape))\n",
    "            #print('MLP Label',train['price_doc'].head())\n",
    "            \n",
    "            features = [x for x in train.columns if x not in ['id', 'price_doc']]\n",
    "            train_x = np.array(train[features])\n",
    "            print(target_mlp.head(n=2))\n",
    "            train_y = np.array(target_mlp)\n",
    "        else:\n",
    "            raise Exception(\"Correct value of 'encoding' is required. Possible values of encoding=['one-hot', 'label']\")\n",
    "        features = [x for x in train.columns if x not in ['id', 'price_doc']]\n",
    "        \n",
    "        '''train_x = np.array(train[features])\n",
    "        print(train['price_doc'].head(n=2))\n",
    "        train_y = np.array(train['price_doc'])\n",
    "        '''\n",
    "        x_tr, x_te, y_tr, y_te = train_test_split(train_x, train_y, test_size=self.test_size, random_state=self.seed)\n",
    "        return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "    def predict_folds(self, model_func, xtrain, ytrain, fit_kwargs={}, predict_kwargs={}):\n",
    "        folds = KFold(len(ytrain), shuffle=False, n_folds=3)\n",
    "        fold_preds = np.zeros(len(ytrain))\n",
    "        fold_shape = fold_preds.shape\n",
    "        for k, (train_index, test_index) in enumerate(folds):\n",
    "            xtr = xtrain[train_index]\n",
    "            ytr = ytrain[train_index]\n",
    "            estimator = model_func(xtrain.shape[1])\n",
    "            xte, yte = xtrain[test_index], ytrain[test_index]\n",
    "            estimator.fit(xtr, ytr, **fit_kwargs)\n",
    "            fold_preds[test_index] = np.reshape(estimator.predict(xte, **predict_kwargs),-1)\n",
    "        return fold_preds\n",
    "\n",
    "    def predict_holdout(self, model_func, xtrain, ytrain, xtest, fit_kwargs={}, predict_kwargs={}):\n",
    "        estimator = model_func(xtrain.shape[1])\n",
    "        estimator.fit(xtrain, ytrain, **fit_kwargs)\n",
    "        return estimator.predict(xtest, **predict_kwargs)\n",
    "\n",
    "    def evaluate_estimators(self, xgb_pred, mlp_pred, test_y):\n",
    "        return (mean_squared_error((xgb_pred), (test_y))), (mean_squared_error((mlp_pred),(test_y)))\n",
    "\n",
    "    def stack(self, xgb_oof, mlp_oof, oof_y):\n",
    "        assert len(xgb_oof) == len(mlp_oof)\n",
    "        oof_x = np.vstack((xgb_oof, mlp_oof)).T\n",
    "        metaestimator = LinearRegression()\n",
    "        metaestimator.fit(oof_x, oof_y)\n",
    "        return metaestimator\n",
    "\n",
    "    def evaluate_stacker(self, stacker, xgb_pred, mlp_pred, holdout_y):\n",
    "        holdout_pred = np.hstack((xgb_pred.reshape(len(xgb_pred), 1), np.array(mlp_pred)))\n",
    "        predictions = stacker.predict(holdout_pred)\n",
    "        score = mean_squared_error((predictions), (holdout_y))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds = [10,20,30,40,50]\n",
    "#seeds = [0]\n",
    "final_scores = []\n",
    "\n",
    "# Preparing XGBoost model function\n",
    "xgbst = lambda _: xgb.XGBRegressor(num_boost_round=100,\n",
    "                           eta=0.1,\n",
    "                           eval_metric='rmse',\n",
    "                           objective='reg:linear',\n",
    "                           seed=0,\n",
    "                           gamma=0,\n",
    "                           max_depth=8,\n",
    "                           min_child_weight=6,\n",
    "                           colsample_bytree=0.6,\n",
    "                           subsample=0.9)\n",
    "\n",
    "# Preparing MLP model function. We need to pass input dimension into\n",
    "# this function, which is done in Stacker class\n",
    "def hyper_model(dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim =dim,init='normal' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    # Compile Model\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_fit_params = {'epochs': 100, 'batch_size': 32, 'verbose': 1}\n",
    "mlp_pred_params = {'batch_size': 32, 'verbose': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n",
      "C:\\Continuum\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=353, kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 8min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture cap --no-stderr \n",
    "\n",
    "for seed in seeds:\n",
    "    stacker = Stacker(xgbst, hyper_model, seed=seed,\n",
    "                  mlp_fit_kwargs=mlp_fit_params, mlp_predict_kwargs=mlp_pred_params)\n",
    "    score = stacker.stack_and_compare()\n",
    "    final_scores.append(score)\n",
    "    print (\"Seed {} completed with scores :: {}\".format(seed, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stacker_validation.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 10\n",
      "SCORE: {'stacker': 2193287.6247396455, 'xgb': 2207972.477460864, 'mlp': 2466970.5167615437}\n",
      "SEED: 20\n",
      "SCORE: {'stacker': 2171958.4140811809, 'xgb': 2178806.3050570297, 'mlp': 2479116.8569160611}\n",
      "SEED: 30\n",
      "SCORE: {'stacker': 2255960.6154502756, 'xgb': 2255339.2048879955, 'mlp': 2594524.342251874}\n",
      "SEED: 40\n",
      "SCORE: {'stacker': 2196563.2986961356, 'xgb': 2202009.8203167059, 'mlp': 2517315.4996815301}\n",
      "SEED: 50\n",
      "SCORE: {'stacker': 2236356.0243481873, 'xgb': 2246863.4458257603, 'mlp': 2576243.6124122702}\n"
     ]
    }
   ],
   "source": [
    "for seed, score in zip(seeds, final_scores):\n",
    "    print (\"SEED: {}\".format(seed))\n",
    "    print (\"SCORE: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mlp</th>\n",
       "      <th>stacker</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.526834e+06</td>\n",
       "      <td>2.210825e+06</td>\n",
       "      <td>2.218198e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.695274e+04</td>\n",
       "      <td>3.431699e+04</td>\n",
       "      <td>3.209167e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.466971e+06</td>\n",
       "      <td>2.171958e+06</td>\n",
       "      <td>2.178806e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.479117e+06</td>\n",
       "      <td>2.193288e+06</td>\n",
       "      <td>2.202010e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.517315e+06</td>\n",
       "      <td>2.196563e+06</td>\n",
       "      <td>2.207972e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.576244e+06</td>\n",
       "      <td>2.236356e+06</td>\n",
       "      <td>2.246863e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.594524e+06</td>\n",
       "      <td>2.255961e+06</td>\n",
       "      <td>2.255339e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mlp       stacker           xgb\n",
       "count  5.000000e+00  5.000000e+00  5.000000e+00\n",
       "mean   2.526834e+06  2.210825e+06  2.218198e+06\n",
       "std    5.695274e+04  3.431699e+04  3.209167e+04\n",
       "min    2.466971e+06  2.171958e+06  2.178806e+06\n",
       "25%    2.479117e+06  2.193288e+06  2.202010e+06\n",
       "50%    2.517315e+06  2.196563e+06  2.207972e+06\n",
       "75%    2.576244e+06  2.236356e+06  2.246863e+06\n",
       "max    2.594524e+06  2.255961e+06  2.255339e+06"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7662, 353), (7662, 208))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_mlp_X.shape, final_test_xgb_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7662,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb predictions\n",
    "final_xgb_pred_y = reg_xgb.predict(np.array(final_test_xgb_X))\n",
    "final_xgb_pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7662, 1), (7662,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlp predictions\n",
    "\n",
    "final_mlp_pred_y=reg_mlp.predict(final_test_mlp_X.as_matrix(), batch_size=32, verbose=0)\n",
    "final_mlp_pred_y_rs = np.reshape(final_mlp_pred_y, final_mlp_pred_y.shape[0])\n",
    "final_mlp_pred_y.shape, final_mlp_pred_y_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = 0.80887096\n",
    "B = 0.23771801\n",
    "\n",
    "final_stacker_pred_y = A*final_xgb_pred_y +  B* final_mlp_pred_y_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs=pd.DataFrame({'id':final_test_mlp_id_test.as_matrix(),'price_doc':final_stacker_pred_y})\n",
    "subs.head()\n",
    "subs.to_csv(\"output/final_stacker_pred_20170819.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The final submission gave score of 0.32622, this the best score achieved compared to all the indidual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
